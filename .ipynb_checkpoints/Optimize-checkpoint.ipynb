{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from Functions import *\n",
    "from Optimizations import *\n",
    "from utils import *\n",
    "from Saving import *\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = flat_sharp_gaussian\n",
    "grad_f = grad_flat_sharp_gaussian\n",
    "\n",
    "x_range = [-25, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1]])\n",
    "b = np.array([0])\n",
    "f = QuadraticFunctionInit(A, b)\n",
    "grad_f = GradQuadraticFunctionInit(A)\n",
    "\n",
    "x_range = [-15, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximating density with the particles\n",
    "def V(x, K, particles):\n",
    "    N = len(particles)\n",
    "    ret_sum = 0\n",
    "    for p in particles:\n",
    "        ret_sum += K(x, p)\n",
    "    return 1 / float(N) * ret_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_positions_softmax(weights, positions, beta=1):\n",
    "    probabilities = softmax(weights, beta)\n",
    "    pos_filter = np.random.choice(list(range(len(positions))), len(positions), p=probabilities)\n",
    "    return np.array(positions)[np.array(pos_filter)]\n",
    "\n",
    "def softmax(weights, beta=1):\n",
    "    sum_exp_weights = sum([np.exp(beta*w) for w in weights])\n",
    "    probabilities = np.array([np.exp(beta*w) for w in weights]) / sum_exp_weights\n",
    "    return probabilities\n",
    "\n",
    "def weight_function_discounted_norm(U, grad_U, x, curr_weights, gamma=1):\n",
    "    return gamma * curr_weights + np.linalg.norm(grad_U(x.T), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_params = [[-10, 1, -2], [0, 10, -1], [10, 3, 1], [-20, 1, 3], [20, 1, 3]]\n",
    "f = gaussian_sum(g_params)\n",
    "grad_f = grad_gaussian_sum(g_params)\n",
    "\n",
    "x_range = [-20, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_process(process):\n",
    "    # get potential_function and gradient\n",
    "    if process[\"potential_function\"][\"name\"] == \"gaussian\":\n",
    "        params = process[\"potential_function\"][\"params\"]\n",
    "        f = gaussian_sum(g_params)\n",
    "        grad_f = grad_gaussian_sum(g_params)    \n",
    "    else:\n",
    "        raise ValueError(\"Does not support given function {}\".format(process[\"name\"]))\n",
    "    \n",
    "    # get weight_function \n",
    "    if process[\"weight_function\"][\"name\"] == \"discounted_norm\":\n",
    "        params = process[\"weight_function\"][\"params\"]\n",
    "        weight_function = lambda U, grad_U, x, curr_weights: weight_function_discounted_norm(U, grad_U, x, curr_weights, params[\"gamma\"])\n",
    "    else:\n",
    "        raise ValueError(\"Does not support given function {}\".format(process[\"weight_function\"][\"name\"]))\n",
    "    \n",
    "    # get resample_function\n",
    "    if process[\"resample_function\"][\"name\"] == \"softmax\":\n",
    "        params = process[\"resample_function\"][\"params\"]\n",
    "        resample_function = lambda w, end_p: resample_positions_softmax(w, end_p, beta=params[\"beta\"])\n",
    "    elif process[\"resample_function\"][\"name\"] == \"none\":\n",
    "        params = process[\"resample_function\"][\"params\"]\n",
    "        resample_function = lambda w, end_p: end_p\n",
    "    else:\n",
    "        raise ValueError(\"Does not support given function {}\".format(process[\"resample_function\"][\"name\"]))\n",
    "        \n",
    "    # get domain_enforcer \n",
    "    x_range = process[\"x_range\"]\n",
    "    if process[\"domain_enforcer\"][\"name\"] == \"hyper_cube_enforcer\":\n",
    "        params = process[\"hyper_cube_enforcer\"][\"params\"]\n",
    "        init_hyper_cube_enforcer = hyper_cube_enforcer(x_range[0], x_range[1], params[\"strength\"])\n",
    "    else:\n",
    "        raise ValueError(\"Does not support given function {}\".format(process[\"weight_function\"][\"name\"]))\n",
    "        \n",
    "        \n",
    "    return diffusion_resampling(f, grad_f, \n",
    "                                     process, process[\"total_iter\"], process[\"tau\"], \n",
    "                                     domain_enforcer=init_hyper_cube_enforcer)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 1000\n",
    "process = {}\n",
    "process[\"start\"] = [[np.random.uniform(x_range[0], x_range[1])] for _ in range(num_particles)]\n",
    "process[\"gamma\"] = lambda t: 2.5\n",
    "process[\"temperature\"] = lambda t: 1\n",
    "process[\"epsilon\"] = 0\n",
    "process[\"weight_function\"] = {\"name\": \"discounted_norm\", \"params\": {\"gamma\" : 0.999}} # weight_function # \n",
    "process[\"resample_function\"] = {\"name\": \"softmax\", \"params\": {\"beta\": -1}} # lambda w, end_p: resample_positions_softmax(w, end_p, beta=-0.1) # \n",
    "# process[\"resample_function\"] =  lambda w, end_p: end_p # \n",
    "\n",
    "process[\"potential_function\"] = {\"name\": \"gaussian\", \"params\": g_params}\n",
    "process[\"domain_enforcer\"] = {\"name\": \"hyper_cube_enforcer\", \"params\": {\"strength\": 0.2}\n",
    "process[\"total_iter\"] = 20\n",
    "process[\"tau\"] = 50\n",
    "process[\"x_range\"] = x_range\n",
    "\n",
    "\n",
    "all_paths = run_process(process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(x_range[0], x_range[1], 200)\n",
    "inp = np.array([X])\n",
    "Y = f(inp)\n",
    "\n",
    "all_paths_proc = []\n",
    "\n",
    "for t in range(len(all_paths)):\n",
    "    curr_paths = all_paths[t]\n",
    "    \n",
    "    all_paths_proc.append([])\n",
    "    \n",
    "    for p in range(len(curr_paths)):\n",
    "        x_curr = np.array(curr_paths[p])\n",
    "        out = f(x_curr.T)\n",
    "        all_paths_proc[-1].append(np.concatenate([x_curr, out.reshape(len(out), 1)], axis=1))\n",
    "\n",
    "all_paths_proc = np.array(all_paths_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = multi_gaussian(np.array([[0.6]]))\n",
    "\n",
    "\n",
    "ani_path = create_animation_1d_pictures_particles(all_paths_proc, X, Y, graph_details={\"p_size\": 3, #\"density_function\": None})\n",
    "                                                                                      \"density_function\": \n",
    "                                                                                      lambda x, p: V(np.array([x]), K, p)})\n",
    "\n",
    "create_animation(ani_path, \"test.mp4\", framerate=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to note:\n",
    "\n",
    "The beta value of the softmax turns out to be very important. Adjusting that value determines how much to value each respective shallow/flat regions. Seems like changing it can give you a nice stationary distribution around certain falt minima. \n",
    "\n",
    "Also the tau value is important. Letting the run run for too long will cause the particles to approach more or less the stationary distribution if ran with soley diffusion. \n",
    "\n",
    "Maybe penalize where you started from. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Simulate the feynman kac method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
